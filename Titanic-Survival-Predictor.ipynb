{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is a breakdown of the kaggle titanic competicion, this is a chance for me to analize data, to then ad a machine learning module to estimate the survival of passengers based on the provived data of the passengers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  -------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring Imports & Bringing In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time, random, datetime\n",
    "\n",
    "# Importing these for manipulating data \n",
    "import numpy, pandas\n",
    "\n",
    "# And importing these for visualising\n",
    "import seaborn, matplotlib.pyplot as ploter, missingno\n",
    "\n",
    "ploter.style.use('seaborn-whitegrid')\n",
    "\n",
    "# SKLearn preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, label_binarize\n",
    "\n",
    "# Importing some Machine learning modules\n",
    "import catboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, tree, preprocessing, metrics, linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier, Pool, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bringing in data and storing them as a \n",
    "train = pandas.read_csv('./Data/train.csv')\n",
    "test = pandas.read_csv('./Data/test.csv')\n",
    "gender_submission = pandas.read_csv('./Data/gender_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Look\n",
    "A breif look at the existing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling to view the head (first 5 rows of data) of the training data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as training data -- looking at first 5 rows of data.\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now having a look at the provided example submisison dataframe\n",
    "gender_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Displays a bunch of handy statistics real easy.\n",
    "# Ie, We see that the highest age was 80 years old, people had on average half a sibling/spouse, etc.\n",
    "# Also we get a glimps that not all passengers had their age accounted for... so we have missing data that will perhaps impact our models design\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "After having quick look, I found holes in the data. Time to really see what we're looking at (missing data wise) and have a deeper look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, this will plot a graph of missing values - to better see what data we have missing.\n",
    "missingno.matrix(train, figsize = (30,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin = pandas.DataFrame() \n",
    "df_con = pandas.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays what are variable types my data columns that im working with.  \n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing how many people survived and how many unfortunatly didnt?\n",
    "fig = ploter.figure(figsize=(20,1))\n",
    "seaborn.countplot(y='Survived', data=train);\n",
    "print(train.Survived.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender/Sex\n",
    "A look at the data in regards to the sex of the passengers and maybe find if their's a connection to their survival rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin['Survived'] = train['Survived']\n",
    "df_con['Survived'] = train['Survived']\n",
    "df_bin['Pclass'] = train['Pclass']\n",
    "df_con['Pclass'] = train['Pclass']\n",
    "df_bin['Sex'] = train['Sex']\n",
    "df_bin['Sex'] = numpy.where(df_bin['Sex'] == 'female', 1, 0)\n",
    "df_con['Sex'] = train['Sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see how the sex/gender variable looks compaired to survival. \n",
    "fig = ploter.figure(figsize=(10, 10))\n",
    "seaborn.distplot(df_bin.loc[df_bin['Survived'] == 1]['Sex'], kde_kws={'label': 'Survived'});\n",
    "seaborn.distplot(df_bin.loc[df_bin['Survived'] == 0]['Sex'], kde_kws={'label': 'Did not survive'});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data here shows me that not many survived, but more of the survivers were female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count_dist(data, bin_df, label_column, target_column, figsize=(20, 5), use_bin_df=False):\n",
    "    if use_bin_df: \n",
    "        fig = ploter.figure(figsize=figsize)\n",
    "        ploter.subplot(1, 2, 1)\n",
    "        seaborn.countplot(y=target_column, data=bin_df);\n",
    "        ploter.subplot(1, 2, 2)\n",
    "        seaborn.distplot(data.loc[data[label_column] == 1][target_column], \n",
    "                     kde_kws={\"label\": \"Survived\"});\n",
    "        seaborn.distplot(data.loc[data[label_column] == 0][target_column], \n",
    "                     kde_kws={\"label\": \"Did not survive\"});\n",
    "    else:\n",
    "        fig = ploter.figure(figsize=figsize)\n",
    "        ploter.subplot(1, 2, 1)\n",
    "        seaborn.countplot(y=target_column, data=data);\n",
    "        ploter.subplot(1, 2, 2)\n",
    "        seaborn.distplot(data.loc[data[label_column] == 1][target_column], \n",
    "                     kde_kws={\"label\": \"Survived\"});\n",
    "        seaborn.distplot(data.loc[data[label_column] == 0][target_column], \n",
    "                     kde_kws={\"label\": \"Did not survive\"});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siblings & Spouses\n",
    "Looking at the avalible data of the Siblings & Spouse, compairing them to survival and analyising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many missing values does SibSp have?\n",
    "train.SibSp.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many Siblings &/ Spouses did passengers have aboard\n",
    "train.SibSp.value_counts()\n",
    "# IE, we now see that there were 608 people that were alone on their travels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SibSp to subset dataframes\n",
    "df_bin['SibSp'] = train['SibSp']\n",
    "df_con['SibSp'] = train['SibSp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairing how passengers survived when they had siblings/spouses.\n",
    "plot_count_dist(train, \n",
    "                bin_df=df_bin, \n",
    "                label_column='Survived', \n",
    "                target_column='SibSp', \n",
    "                figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data above, we see that more couples had a chance of surviving.\n",
    "\n",
    "It's this kind of data that can help train out ML model to make assumtions of survivers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parents & Children\n",
    "Looking athe number of Parents & Children aboard the titanic.\n",
    "\n",
    "I'll be running very simular analysis to the Siblings &/ Spouses data!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values for Parch...\n",
    "train.Parch.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Parch.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin['Parch'] = train['Parch']\n",
    "df_con['Parch'] = train['Parch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count_dist(train, \n",
    "                bin_df=df_bin,\n",
    "                label_column='Survived', \n",
    "                target_column='Parch', \n",
    "                figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tickets\n",
    "Looking at the ticket data: the number of types, etc.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any missing data?\n",
    "train.Ticket.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many kinds of ticket are there?\n",
    "seaborn.countplot(y=\"Ticket\", data=train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fare\n",
    "~ Cost of tickets\n",
    "\n",
    "Having a look at the fare data and trying to draw a connection to the survival rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many missing data in the Fare variable?\n",
    "train.Fare.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many different values of Fare are there?\n",
    "seaborn.countplot(y=\"Fare\", data=train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Fare to sub dataframes\n",
    "df_con['Fare'] = train['Fare'] \n",
    "df_bin['Fare'] = pandas.cut(train['Fare'], bins=5) # discretised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do our Fare bins look like?\n",
    "df_bin.Fare.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the Fare bin counts as well as the Fare distribution versus Survived.\n",
    "plot_count_dist(data=train,\n",
    "                bin_df=df_bin,\n",
    "                label_column='Survived', \n",
    "                target_column='Fare', \n",
    "                figsize=(20,10), \n",
    "                use_bin_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embarked\n",
    "Where passengers embarked on the titanic\n",
    "\n",
    "<b>Legend:</b>\n",
    "\n",
    "S -> Southampton\n",
    "\n",
    "C -> Cherbourg\n",
    "\n",
    "Q -> Queenstown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many missing values does Embarked have?\n",
    "train.Embarked.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What kind of values are in Embarked?\n",
    "train.Embarked.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do the counts look like?\n",
    "seaborn.countplot(y='Embarked', data=train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Embarked to sub dataframes\n",
    "df_bin['Embarked'] = train['Embarked']\n",
    "df_con['Embarked'] = train['Embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Embarked rows which are missing values\n",
    "print(len(df_con))\n",
    "df_con = df_con.dropna(subset=['Embarked'])\n",
    "df_bin = df_bin.dropna(subset=['Embarked'])\n",
    "print(len(df_con))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding features\n",
    "Now we have our two sub dataframes ready. We can encode the features so they're ready to be used with our machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode binned variables\n",
    "one_hot_cols = df_bin.columns.tolist()\n",
    "one_hot_cols.remove('Survived')\n",
    "df_bin_enc = pandas.get_dummies(df_bin, columns=one_hot_cols)\n",
    "\n",
    "df_bin_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_con.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the categorical columns\n",
    "df_embarked_one_hot = pandas.get_dummies(df_con['Embarked'], \n",
    "                                     prefix='embarked')\n",
    "\n",
    "df_sex_one_hot = pandas.get_dummies(df_con['Sex'], \n",
    "                                prefix='sex')\n",
    "\n",
    "df_plcass_one_hot = pandas.get_dummies(df_con['Pclass'], \n",
    "                                   prefix='pclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the one hot encoded columns with df_con_enc\n",
    "df_con_enc = pandas.concat([df_con, \n",
    "                        df_embarked_one_hot, \n",
    "                        df_sex_one_hot, \n",
    "                        df_plcass_one_hot], axis=1)\n",
    "\n",
    "# Drop the original categorical columns (because now they've been one hot encoded)\n",
    "df_con_enc = df_con_enc.drop(['Pclass', 'Sex', 'Embarked'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_con_enc.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Machine Learning Models\n",
    "Now our data has been manipulating and converted to numbers, we can run a series of different machine learning algorithms over it to find which yield the best results.\n",
    "\n",
    "First, lets seperate the data!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the dataframe I'll use for my fire prediction\n",
    "selected_df = df_con_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into data and labels\n",
    "X_train = selected_df.drop('Survived', axis=1) # data\n",
    "y_train = selected_df.Survived # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape # without the lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Algorithims\n",
    "Creating a function that will check the accuracy of the different algorithims that get passed through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that runs the requested algorithm and returns the accuracy metrics\n",
    "def check_accuracy(algo, X_train, y_train, cv):\n",
    "    \n",
    "    # One Pass\n",
    "    model = algo.fit(X_train, y_train)\n",
    "    acc = round(model.score(X_train, y_train) * 100, 2)\n",
    "    \n",
    "    # Cross Validation \n",
    "    train_pred = model_selection.cross_val_predict(algo, \n",
    "                                                  X_train, \n",
    "                                                  y_train, \n",
    "                                                  cv=cv, \n",
    "                                                  n_jobs = -1)\n",
    "    # Cross-validation accuracy metric\n",
    "    acc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\n",
    "    \n",
    "    return train_pred, acc, acc_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "start_time = time.time()\n",
    "train_pred_log, acc_log, acc_cv_log = check_accuracy(LogisticRegression(), \n",
    "                                                               X_train, \n",
    "                                                               y_train, \n",
    "                                                                    10)\n",
    "log_time = (time.time() - start_time)\n",
    "print(\"Accuracy: %s\" % acc_log)\n",
    "print(\"Accuracy CV 10-Fold: %s\" % acc_cv_log)\n",
    "print(\"Running Time: %s\" % datetime.timedelta(seconds=log_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Nearest Neighbours\n",
    "start_time = time.time()\n",
    "train_pred_knn, acc_knn, acc_cv_knn = check_accuracy(KNeighborsClassifier(), \n",
    "                                                  X_train, \n",
    "                                                  y_train, \n",
    "                                                  10)\n",
    "knn_time = (time.time() - start_time)\n",
    "print(\"Accuracy: %s\" % acc_knn)\n",
    "print(\"Accuracy CV 10-Fold: %s\" % acc_cv_knn)\n",
    "print(\"Running Time: %s\" % datetime.timedelta(seconds=knn_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "start_time = time.time()\n",
    "train_pred_gaussian, acc_gaussian, acc_cv_gaussian = check_accuracy(GaussianNB(), \n",
    "                                                                      X_train, \n",
    "                                                                      y_train, \n",
    "                                                                           10)\n",
    "gaussian_time = (time.time() - start_time)\n",
    "print(\"Accuracy: %s\" % acc_gaussian)\n",
    "print(\"Accuracy CV 10-Fold: %s\" % acc_cv_gaussian)\n",
    "print(\"Running Time: %s\" % datetime.timedelta(seconds=gaussian_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC\n",
    "start_time = time.time()\n",
    "train_pred_svc, acc_linear_svc, acc_cv_linear_svc = check_accuracy(LinearSVC(),\n",
    "                                                                X_train, \n",
    "                                                                y_train, \n",
    "                                                                10)\n",
    "linear_svc_time = (time.time() - start_time)\n",
    "print(\"Accuracy: %s\" % acc_linear_svc)\n",
    "print(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_svc)\n",
    "print(\"Running Time: %s\" % datetime.timedelta(seconds=linear_svc_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "start_time = time.time()\n",
    "train_pred_sgd, acc_sgd, acc_cv_sgd = check_accuracy(SGDClassifier(), \n",
    "                                                  X_train, \n",
    "                                                  y_train,\n",
    "                                                  10)\n",
    "sgd_time = (time.time() - start_time)\n",
    "print(\"Accuracy: %s\" % acc_sgd)\n",
    "print(\"Accuracy CV 10-Fold: %s\" % acc_cv_sgd)\n",
    "print(\"Running Time: %s\" % datetime.timedelta(seconds=sgd_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "start_time = time.time()\n",
    "train_pred_dt, acc_dt, acc_cv_dt = check_accuracy(DecisionTreeClassifier(), \n",
    "                                                                X_train, \n",
    "                                                                y_train,\n",
    "                                                                10)\n",
    "dt_time = (time.time() - start_time)\n",
    "print(\"Accuracy: %s\" % acc_dt)\n",
    "print(\"Accuracy CV 10-Fold: %s\" % acc_cv_dt)\n",
    "print(\"Running Time: %s\" % datetime.timedelta(seconds=dt_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Trees\n",
    "start_time = time.time()\n",
    "train_pred_gbt, acc_gbt, acc_cv_gbt = check_accuracy(GradientBoostingClassifier(), \n",
    "                                                                       X_train, \n",
    "                                                                       y_train,\n",
    "                                                                       10)\n",
    "gbt_time = (time.time() - start_time)\n",
    "print(\"Accuracy: %s\" % acc_gbt)\n",
    "print(\"Accuracy CV 10-Fold: %s\" % acc_cv_gbt)\n",
    "print(\"Running Time: %s\" % datetime.timedelta(seconds=gbt_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost Algorithm\n",
    "CatBoost is a state-of-the-art open-source gradient boosting on decision trees library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
